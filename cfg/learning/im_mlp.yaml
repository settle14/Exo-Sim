  # cfg/learning/im_mlp.yaml
  agent_name: agent_im
  actor_type: gauss          # <— 高斯策略，已对齐你的实现

  # Main training loop max epoch
  max_epoch: 100000000
  # PPO steps per epoch
  opt_num_epochs: 8

  # Trajectory samples per epoch
  min_batch_size: 25600

  # Maximum gradient norm for PPO
  policy_grad_clip: 25

  # Latest model save frequency
  save_curr_frequency: 50

  # Model save frequency
  save_frequency: 1000

  # Termination history flag
  resume_history: True

  # Dynamic std flag
  fix_std: False

  # Policy network log std
  log_std: 0

  # GAE discount factor
  gamma: 0.99

  # GAE tau
  tau: 0.95

  # PPO epsilon
  clip_epsilon: 0.18

  # Observation clipping
  clip_obs: False
  clip_obs_range: [-5.0, 5.0]

  policy_optimizer: "adam"
  policy_weightdecay: 0.0
  policy_lr: 3.125e-5
  value_optimizer: "adam"
  value_weightdecay: 0.0
  value_lr: 1.875e-4

  # network/lr
  mlp:
    units: [2048, 1536, 1024, 1024, 512, 512]
    activation: silu

    initializer:
      name: default
    regularizer:
      name: None

  composer:
    units: [1024, 512]
    activation: silu

  moe:
    units: [1024, 512, 256]
    activation: silu

  masking_prob: 0.0
