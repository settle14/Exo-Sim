# Simplified training configuration for forward walking
# Always set to agent_im
agent_name: "agent_im"

# Main training loop max epoch
max_epoch: 100000000

# RL policy type - use simple gaussian
actor_type: "gauss"

# PPO steps per epoch
opt_num_epochs: 10

# Trajectory samples per epoch - reduced for stability
min_batch_size: 12800

# Maximum gradient norm for PPO
policy_grad_clip: 25

# Latest model save frequency
save_curr_frequency: 50

# Model save frequency
save_frequency: 1000

# Termination history flag
resume_history: True

# Dynamic std flag
fix_std: False

# Policy network log std
log_std: 0

# GAE discount factor
gamma: 0.99

# GAE tau
tau: 0.95

# PPO epsilon
clip_epsilon: 0.2

# Observation clipping
clip_obs: False
clip_obs_range: [-5.0, 5.0]

policy_optimizer: "adam"
policy_weightdecay: 0.0
policy_lr: 3.e-5

value_optimizer: "adam"
value_weightdecay: 0.0
value_lr: 3.e-4

mlp:
  units: [1024, 768, 512, 256]
  activation: silu

  initializer:
    name: default
  regularizer:
    name: None

composer:
  units: [512, 256]
  activation: silu

moe:
  units: [512, 256, 128]
  activation: silu

masking_prob: 0.0
